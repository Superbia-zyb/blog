---
title: 机器学习 | 决策树
date: 2021-02-19 14:50:59
tags:
---



整理了一下机器学习中，决策树的内容，做了一篇笔记。

#### 决策树

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/15.png" alt="15" style="zoom:80%;" />

决策树是一种基本的分类与回归方法 它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。

决策树是一种自上而下，对样本数据进行树形分类的过程，由节点和有向边组成。节点分为内部节点和叶节点，每个内部节点表示一个特征或属性，叶节点表示类别，边代表划分的条件。从顶部节点开始，所有样本聚在一起，经过根节点的划分，样本被分到不同的子节点中，再根据子节点的特征进一步划分，直至所有样本都被归到某个类别。

构建决策树就是一个递归的选择内部节点，计算划分条件的边，最后到达叶子节点的过程。

<!--more-->

#### 特征选择

##### 信息量：

一个事件 $x_i$ 发生的概率为 $P(x_i)$ 那么信息量定义为 : $l(x_i)=-log(p(x_i))$

事件发生的概率越大 带来的信息量就越小

##### 熵： 度量随机变量的不确定性

设 $X$ 为取有限个数的离散型随机变量 且 $P(X=x_i)=p_i$

则： $H(x)= -\sum\limits_{i=1}^{n}p_il_i= -\sum\limits_{i=1}^{n}p_i \log{p_i}$ . 因为只与 $X$ 的分布有关,也记作 $H(p)$

分布为伯努利分布时,即$\begin{cases}P(X=1)=p \\P(x=0)=1-p \end{cases} , \ 0\le p \le 1$ , $H(p)=-p\log_2p-(1-p)log_2(1-p)$      

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/1.png" alt="1" style="zoom:85%;" />

信息熵是计算所有类别的所有可能值包含的信息期望 

##### 条件熵

设有随机变量$(X,Y)$ 其联合概率分布为 
$$
P(X=x_i,Y=y_i)=p_{i,j}, \ \ i=1,2,...n; \ \ j=1,2,...,m;
$$
条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性. 

随机变量 $X$ 给定的条件下随机变量 $Y$ 的条件熵 $H(Y|X)$ 定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望

$$
H(Y|X)=\sum\limits_{i=1}^np_iH(Y|X=x_i) \ ,\ p_i=P(X=x_i)\ ,\ i=1,2,...,n
$$

当熵,条件熵中的概率有数据估计得到时,所对应的熵称作经验熵,条件熵称作经验条件熵.
##### 信息增益

信息增益表示: 知道特征 $X$ 的信息而使得类 $Y$ 的信息的不确定性减少的程度.

特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$ ,定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差,即：
$$
g(D,A)=H(D)-H(D|A)
$$
不同的特征往往具有不同的信息增益,信息增益大的特征具有更强的分类能力

设训练数据集为  $|D|$ 表示其样本容量(个数) . 设有 $K$ 个类 $C_k ,\ \ k=1,2,...,K $ , $|C_k|$ 为属于类 $C_k$ 的样本个数 $\sum\limits_{k=1}^{K}|C_k|=|D| $  .设特征 $A$ 有 $n$ 个不同的取值 ${a_1,...a_n}$ , 根据特征 $A$ 的取值将 $D$ 划分为 $n$ 个自己 $D_1...D_n$ , $|D_i|$ 为 $|D_i|$的样本个数 $\sum\limits_{i=1}^{n}|D_i|=|D|$ .记自己 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$ ,  记子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$ 即 $D_{ik}=D_i \cap C_k  , $$|D_{ik}|$ 为 $D_{ik} $的样本个数. 

则: 数据集 $D$ 的经验熵 $H(D)$为： 
$$
H(D)=-\sum\limits_{k=1}^{K}\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}
$$

特征值 $A$ 对数据集 $D$ 的经验条件熵 $H(D|A)$ 为： 
$$
H(D|A)=\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}\sum\limits_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}
$$

信息增益：$g(D,A)=H(D)-H(D|A)$

信息增益越大 则意味使用特征值 $A$ 来进行划分的效果越好

##### 信息增益比

特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比,即 
$$
\begin{align}
&g_R(D,A)=\frac{g(D,A)}{H_A(D)} \\ 
&H_A(D)=-\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|} \log_2\frac{|D_i|}{|D|}
\end{align}
$$

#### 决策树的生成

##### ID3 算法
按照信息增益来递归建树:

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/3.png" alt="3" style="zoom:85%;"  />

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/4.png" alt="4" style="zoom:80%;"  />

ID3算法的缺点是：信息增益偏向取值较多的特征

原因：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，信息增益更偏向取值较多的特征。

##### C4,5 算法

按照信息增益比来递归建树:

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/2.png" alt="2" style="zoom:80%;"  />

#### 决策树的剪枝

##### 过拟合

以线性回归与逻辑斯蒂回归为例

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/10.png" alt="10" style="zoom:70%;"   />

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/9.png" alt="9" style="zoom:85%;"  />

过度拟合的问题通常发生在变量（特征）过多的时候。这种情况下训练出的方程总是能很好的拟合训练数据，也就是说，我们的损失函数可能非常接近于 0 或者就为 0。

但是，这样的曲线过度拟合训练数据，会导致它无法泛化到新的数据样本中

##### 损失函数

设树 $T$ 的叶节点个数为 $|T|$ ,$t$ 是树 $T$ 的叶节点 该叶节点 $t$ 有 $N_t$ 个样本点,其中 $k$ 类的样本点 $N_{tk}$ 个 , $k=1,2...,K$, $H_{t}(T) $ 为叶节点 $t$ 熵的经验熵,$\alpha \ge 0$ 为参数 则决策树的损失函数可以定义为
$$
\begin{align}
C_{\alpha}(T) &=\sum\limits_{t=1}^{|T|} N_tH_t(T)+\alpha|T| \\
&=\sum\limits_{t=1}^{|T|}\sum\limits_{k=1}^{K}N_{tk}\log\frac{N_{tk}}{N_t}+\alpha|T| \\ &=C(T)+\alpha|T|
\end{align}
$$

$C(T)$ 表示对训练数据的预测误差 ，即模型与训练数据的拟合程度， $|T|$ 表示模型复杂度，参数 $\alpha$ 控制二者之间的影响。较大的 $\alpha$ 促使选择较简单的模型，较小的 $\alpha$ 促使选择较复杂的模型。  $\alpha=0$ 表示只考虑模型与训练数据的拟合程度，不考虑模型的复杂度

这个时候我们求 $C_{\alpha}(T)$ 的最小值，此时一方面要求拟合程度尽可能高，另一方面是希望复杂度尽可能低

但是二者是矛盾的 因为拟合程度越高 意味着模型越复杂，因此 $\alpha$参数 作为调节，起到很重要的作用

##### 预剪枝与后剪枝

预剪枝：在决策树生成过程中 对每个结点划分前先进行估计  若当前节点划分不能带来泛化性能提升 停止划分并标记为叶结点

预剪枝原则有：
1. 节点达到完全纯度；
2. 树的深度达到用户所要的深度；
3. 节点中样本个数少于用户指定个数；
4. 不纯度指标下降的最大幅度小于用户指定的幅度。

后剪枝：从训练集生成一颗完整的决策树，然后自底向上对非叶节点考察 若将该结点对应的子树替换为叶节点能带来泛化性能提升 则将该子树替换为叶节点

（留出法 交叉验证等） 

   <img src="%E5%86%B3%E7%AD%96%E6%A0%91/12.png" alt="12" style="zoom: 80%;"/>    		 <img src="%E5%86%B3%E7%AD%96%E6%A0%91/13.png" alt="13" style="zoom: 80%;" /> 



​										   <img src="%E5%86%B3%E7%AD%96%E6%A0%91/14.png" alt="14" style="zoom: 80%;" />

后剪枝算法相比预剪枝算法会保留更多分支, 泛化性能优于预剪枝决策树 , 但训练时间开销相对大很多



##### 剪枝算法 （ $\alpha$ 已知 ）

计算出每个点的经验熵 ，从叶子节点向父亲节点回缩

设回缩前的整体树为 $T_A$ 回缩后的整体树为 $T_B$ ，若 $C_{\alpha}(T_A) \ge C_{\alpha}(T_B)$ , 说明复杂度在其中起到关键的作用，而损失函数起的作用很小，简单来说就是剪枝前的叶节点并没有很好的提高准确度但是却带来更复杂的树，因此可以考虑剪去这个节点

​                                       	          	<img src="%E5%86%B3%E7%AD%96%E6%A0%91/5.png" alt="5" style="zoom: 80%;" />		

#### CART(二叉树)

##### 生成回归决策树

特征空间划分为 $M$ 个单元 $R_1...R_M$  并每个单元上都具有输出值 $c_m$ 则回归树模型为
$$
f(x)=\sum\limits_{m=1}^Mc_mI(x\in R_m)
$$

使用平方误差：$\sum\limits_{x_i\in R_m} (y_i-f(x_i))^2$ 来表示回归树对训练数据的预测误差,用平方误差最小的原则来求解每个单元的最优输出

单元 $R_m$ 上的 $c_m$ 的最优值 $\hat{c}_ m$ 是 $R_m$ 上所有 $x_i$ 对应的输出 $y_i$ 的均值，即 $\hat{c}_ m = ave(y_i|x_i\in R_m)$


证明： 假设有 $n$ 个变量 $x$ 属于 $R_m$
$$
\begin{align}
\sum\limits_{x_i\in R_m} (y_i-f(x_i))^2 &= \sum\limits_{i=1}^{n}(y_i-c_m)^2 \\ &= n(c_m)^2-(2\sum\limits_{i=1}^{n}y_i)c_m+\sum\limits_{i=1}^{n} y_i^2
\end{align} \\ 解得:当c_m=\frac{1}{n}\sum\limits_{i=1}^{n}y_i 时 ,平方误差取得最小值\ (\sum\limits_{i=1}^{n}y_i^2)-\frac{1}{n}(\sum_{i=1}^{n}y_i)^2
$$

最小二乘回归树生成方法：

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/6.png" alt="6" style="zoom:90%;"  />

##### 生成分类决策树

基尼指数: 分类中假设有 $K$ 个类 样本点属于第 $k$ 类的概率为 $p_k$ 则概率分布的基尼指数定义为
$$
Gini(p)=\sum\limits_{k=1}^{K}p_k(1-p_k)=1-\sum\limits_{k=1}^{K}p_k^2
$$
对于二类分类问题,设样本点属于第一类的概率为 $p$ ,则概率分布的基尼指数为 $Gini(p)=2p(1-p)$

对于给定的样本集合 $D$ ,其基尼指数为
$$
Gini(D)=1-\sum\limits_{k=1}^{K}(\frac{|C_k|}{|D|})^2
$$

假设样本集合 $D$ 根据特征值 $A$ 是否能取某一可能值 $a$ 被分割为 $D_1$ $D_2$两部分 即 
$$
D_1={(x,y)\in D | A(x)=a}\ ,D_2=D-D_1
$$
则在特征值 $A$ 的条件下 集合 $D$ 的基尼指数定义为
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$

基尼指数 $Gini(D)$ 表示集合 $D$ 的不确定性 

基尼指数 $Gini(D,A)$ 表示经过 $A=a$ 的分割后 集合 $D$ 的不确定性

基尼指数直观上反映了从数据集随机抽取两个样本 其类别标记不一致的概率，基尼指数越大 样本的不确定性越大 

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/7.png" alt="7" style="zoom:80%;"  />

##### CART剪枝（$\alpha$ 未知）

$CART$ 剪枝算法由两步组成：

$1) :$ 从 由生成算法产生的决策树 $T_0$ 底端开始剪枝，直到 $T_0$ 的根节点 形成一个子树序列 $\{ T_0,T_1...T_n \}$

剪枝的过程中 计算子树的损失函数：

$$
\begin{align}
C_{\alpha}(T) =C(T)+\alpha|T|
\end{align}
$$

$T$ 为任意子树，$C(T)$ 为对训练数据的预测误差，比如基尼指数，$|T|$ 为子树 $T$ 的叶子节点个数，$\alpha \leq 0$ 为参数，$C_{\alpha}(T)$ 为参数是 $\alpha$ 时的子树 $T$ 的整体损失，$|T|$ 衡量树的复杂度，参数$\alpha$ 权衡训练数据的拟合程度与树的复杂度


对于固定的 $\alpha$ 一定存在使损失函数 $C_{\alpha}(T)$ 最小的子树,表示为 $T_{\alpha}$ ,并且唯一. 当 $\alpha$ 偏大的时候 最优子树 $T_{\alpha}$ 偏小； 当 $\alpha$ 偏小的时候 最优子树 $T_{\alpha}$ 偏大. 当 $\alpha=0$ ，整体树最优. 当 $\alpha \rightarrow 0 $ ，根节点组成的单节点树最优


经证明: 可以使用递归的方法进行剪枝, 将 $\alpha$ 从小增大排序，$ 0 = a_0 < a_1 < ... < a_n < +\infty $ ,产生一系列区间 $ [ a_i,a_{i+1} ) ,i=0,1,..,n$ . 剪枝得到的,与区间 $\alpha \in [a_i , a_{i+1}) ,i=0,1,..,n $  对应的子树序列 $\{ T_0,T_1,...T_n \}$ 序列中的子树是嵌套的

具体： 从整体树 $T_0$ 开始剪枝. 对 $T_0$ 的任意内部节点 $t$ ，以 $t$ 为单节点树的损失函数是: $C_{\alpha}(t)=C(t)+\alpha$ ,以 $t$ 为根节点的子树 $T_t$ 的损失函数是 $C_{\alpha}(T_{t})=C(T_t)+\alpha|T_{t}|$. 当 $\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$, $T_t$ 与 $t$ 有相同的损失函数值， 而 $t$ 的节点更少,因此更可取（**奥卡姆剃刀准则**）,对 $T_t$ 进行剪枝. 

所以 对 $T_0$ 中每一内部节点 $t$ 计算 $g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$, 表示为见之后整体损失函数减少的程度. 在 $T_0$ 中减去 $g(t)$ 最小的 $T_t$ 将得到的子树作为 $T_1$ , 同时将最小的 $g(t)$ 设为 $\alpha_1$. $T_1$ 为区间 $[\alpha_1,\alpha_2)$ 的最优子树

如此反复 直至得到根节点

$2) :$ 通过交叉验证, 在独立的验证数据集上对  $ T_0,T_1,...T_n$ 测试每颗子树的平方误差或基尼指数，平方误差或基尼指数最小的决策树被认为是最优的决策树，当最优决策树确定时， 对应的 $\alpha$ 随之确定。  

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/8.png" alt="8" style="zoom:80%;"  />

##### 交叉验证

<img src="%E5%86%B3%E7%AD%96%E6%A0%91/11.png" alt="11" style="zoom:85%;"    />

##### 缺失值处理

现实中常会遇到不完整样本，即样本某些属性值缺失。

需要解决：

1. 如何在属性值缺失的情况下进行划分属性选择 

2. 给定划分属性 若样本在该属性上的值缺失，如何对样本进行划分

给定训练集 $D$ 和属性 $a$ ，令 $\tilde{D}$ 表示 $D$ 中在属性 $a$ 上没有缺失值的样本子集。

对问题 $1$ , 我们只能通过 $\tilde{D}$  来判断属性 $a$ 的优劣。 假定属性 $a$ 有 $V$ 个可取值 $\{ a^1,a^2...,a^V\}$ , 令 $\tilde{D}^v$ 表示 $\tilde{D}$ 中在属性 $a$ 上取值为 $a^v$ 的样本子集， $\tilde{D}_k$ 表示 $\tilde{D}$ 中属于第 $k$ 类 $(k=1,2,...,|\gamma|)$ 的样本子集， 显然有：$\begin{cases} \tilde{D} = \bigcup_{k=1}^{|\gamma|} \tilde{D} \\ \tilde{D} = \bigcup_{v=1}^{V}\tilde{D}^v  \end{cases}$   . 假设为每个样本 $x$ 赋予一个权值 $w_x$ (根节点中个样本权重初始化为 $1$) 

定义：
$$
\begin{align}
\rho &=\frac{\sum_{x \in \tilde{D}}w_x}{\sum_{x \in D}w_x} \\
\tilde{p}_k &= \frac{\sum_{x \in \tilde{D}_k}w_x}{\sum_{x \in \tilde{D}}w_x} \ \ \ (1\le k \le |\gamma|) \\
\tilde{r}_v &=\frac{\sum_{x\in \tilde{D}^v}w_x}{\sum_{x \in \tilde{D}}w_x} \ \ \ (1\le v \le V)
\end{align}
$$
对于属性 $a$ ,  $\rho$ 表示无缺失值样本所占比例，$\tilde{p}_k$ 表示无缺失值样本中第 $k$ 类所占的比例，$\tilde{r}_v$ 表示无缺失值样本中在属性 $a$ 上取值 $a^v$ 的样本所占的比例 。 显然有：$\begin{cases}\sum_{k=1}^{|\gamma|} \tilde{p} _ {k=1} \\ \sum_{v=1}^{V}\tilde{r} _ {v=1} \end{cases}$

于是 将信息增益的计算式推广为：
$$
\begin{align}
g(D,A)
&=\rho \times g(\tilde{D},A) \\
&=\rho \times (H(\tilde{D})-\sum\limits_{v=1}^{V}\tilde{r} _ v H(\tilde{D}_v)) \\
H(\tilde{D}) &= -\sum\limits_{k=1}^{|\gamma|}\tilde{p} _ k\log_2\tilde{p} _ k
\end{align}
$$
对问题 $2$ , 若样本 $x$ 在划分属性 $a$ 上的取值已知，则将 $x$ 划入与其取值对应的子节点，且样本权值在子结点中保持为 $w_x$ . 若样本 $x$ 在划分属性 $a$ 上的取值未知，则将 $x$ 同时划入所有子节点，且样本权值在与属性值 $a^v$ 对应的子结点中调整为 $\tilde{r} _ v \cdot w_x$ ，即让同一个样本以不同的概率进入到不同的子节点当中去